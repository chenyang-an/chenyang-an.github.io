<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://chenyang-an.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chenyang-an.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-08T07:56:40+00:00</updated><id>https://chenyang-an.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Forward looking: How powerful the foundational models will be at the end of 2026.</title><link href="https://chenyang-an.github.io/blog/2026/prediction/" rel="alternate" type="text/html" title="Forward looking: How powerful the foundational models will be at the end of 2026."/><published>2026-02-07T14:00:00+00:00</published><updated>2026-02-07T14:00:00+00:00</updated><id>https://chenyang-an.github.io/blog/2026/prediction</id><content type="html" xml:base="https://chenyang-an.github.io/blog/2026/prediction/"><![CDATA[<p>Most AI predictions fail for one of two reasons: they are too vague to test, or too dramatic to be useful.</p> <p>This post makes exactly one prediction, and makes it testable.</p> <h2 id="conclusion-first">Conclusion First</h2> <p><strong>Prediction (two parts, same date):</strong> By the end of 2026, frontier foundation model-agent systems will:</p> <ol> <li>Generate research-level math proofs better than the median professional mathematician.</li> <li>Verify math proofs better than the median professional mathematician.</li> </ol> <p>Important scope note: this prediction is about natural-language mathematical proving and reviewing, not formal proof workflows in systems like Lean.</p> <h2 id="why-this-is-a-real-possibility">Why This Is A Real Possibility</h2> <p>In 2025, we saw models move from frequent failure on hard mathematical reasoning tasks to nontrivial success on problems that require long chains of formal thought.</p> <p>A concrete anchor is <a href="https://arxiv.org/abs/2601.23245">arXiv:2601.23245</a>: an agent based on Gemini Deep Think (2025 release) was reported to produce genuinely creative mathematical computation in proving new results.</p> <p>Mathematics is the key domain because it has strict correctness standards:</p> <ul> <li>Statements are explicit.</li> <li>Proofs can be checked step by step.</li> <li>Contradictions eventually surface.</li> </ul> <p>If models can repeatedly produce correct nontrivial proofs and diagnose proof errors with high precision, that is a capability jump, not just stylistic fluency.</p> <p>I also expect the rate of model progress in math to accelerate from 2025 to 2026. The systems are stronger, training and evaluation loops are improving (less bugs), model-assisted data generation and critique create compounding gains, and model can help improve its own training and evaluation pipeline (codex-5.3 helps its own training).</p> <h2 id="part-1-proof-generation-better-than-the-median-mathematician">Part 1: Proof Generation Better Than The Median Mathematician</h2> <p>By December 2026, on hard unseen mathematics problems, frontier agentic systems should:</p> <ol> <li>Produce more correct nontrivial proofs than a randomly sampled professional mathematician under similar time budgets.</li> <li>Recover from failure and repair proof plans through iterative self-critique.</li> <li>Sustain quality across multiple areas (for example algebra, combinatorics, and analysis), not just one benchmark.</li> </ol> <h2 id="why-verification-may-shift-from-humans-to-ai">Why Verification May Shift From Humans To AI</h2> <p>Historically, humans were the final safety layer for mathematical correctness. By end of 2026, that may no longer be true in many workflows.</p> <p>Another key point: if a model can already generate creative, hard, long proofs, it must already have nontrivial internal verification ability. Without some ability to check consistency across many dependent steps, it cannot reliably produce those long-form arguments at all.</p> <h2 id="part-2-proof-verification-better-than-the-median-mathematician">Part 2: Proof Verification Better Than The Median Mathematician</h2> <p>By December 2026, frontier systems should verify proofs better than a randomly sampled professional mathematician, especially on long technical arguments.</p> <p>Reasons this can happen:</p> <ol> <li>AI verifiers can run many independent checking passes at low cost.</li> <li>AI systems can cross-check proofs using different representations (natural language, symbolic steps, and formalized fragments, or use Lean).</li> <li>Verification loops can be automated and repeated until contradictions are exhausted.</li> <li>Human reviewers are sparse, slow, and inconsistent on long technical arguments.</li> </ol> <p>If AI systems become better at both finding subtle errors and avoiding reviewer fatigue, human verification becomes optional for a large fraction of advanced proof workflows.</p> <h2 id="what-would-falsify-this-prediction">What Would Falsify This Prediction</h2> <p>By December 2026, this prediction is wrong if we do <strong>not</strong> see robust evidence for both parts:</p> <ul> <li>Generation: systems do not beat the median professional mathematician on hard unseen proof tasks.</li> <li>Verification: systems do not verify advanced proofs more reliably than the median professional mathematician.</li> <li>Output quality at scale: no flood of AI-generated proofs in 2026 with most outputs solid under technical review.</li> </ul> <p>Failure signs would include:</p> <ul> <li>impressive one-off demos that do not replicate,</li> <li>strong novelty but weak correctness under audit,</li> <li>no clear 2026 surge in AI-generated proofs, or a surge dominated by low-quality proofs,</li> <li>verifier performance collapsing on long or unfamiliar proofs.</li> </ul>]]></content><author><name></name></author><category term="prediction"/><category term="AI"/><category term="prediction"/><category term="reasoning"/><category term="math"/><category term="foundational-models"/><summary type="html"><![CDATA[A forward-looking prediction on frontier model capability in advanced mathematical reasoning and proof verification by the end of 2026]]></summary></entry><entry><title type="html">Paper Analysis: Gemini Contributes Nontrivial Mathematical Insight</title><link href="https://chenyang-an.github.io/blog/2026/math/" rel="alternate" type="text/html" title="Paper Analysis: Gemini Contributes Nontrivial Mathematical Insight"/><published>2026-01-22T15:12:00+00:00</published><updated>2026-01-22T15:12:00+00:00</updated><id>https://chenyang-an.github.io/blog/2026/math</id><content type="html" xml:base="https://chenyang-an.github.io/blog/2026/math/"><![CDATA[<p>This post analyzes a recent pure mathematics work in the field of algebraic geometry (<a href="https://arxiv.org/pdf/2601.07222">arXiv:2601.07222</a>). This work was completed jointly by mathematicians and Gemini.</p> <p>This post will analyze:</p> <ul> <li>The specific role AI played in this work</li> <li>Based on this, assess AI’s capabilities in mathematical proof</li> <li>Predict AI’s development trajectory in mathematical proof, along with technical analysis</li> <li>Suggestions for mathematicians: when writing mathematical works in collaboration with AI, what information should be included to help the community better understand AI’s capabilities in mathematics</li> </ul> <p>I am a scientist working on AI agents and reasoning at a tech company. Before working in AI, I studied mathematics, and the subject happened to be exactly the field of this work (algebraic geometry).</p> <h2 id="conclusion-first">Conclusion First</h2> <p>I believe this work demonstrates that:</p> <p><strong>The SOTA autoregressive transformer of January 2026, combined with agent systems, can produce mathematician-level insights on previously unseen problems.</strong></p> <p><strong>Prediction:</strong> In 2026 (most likely the first half), SOTA autoregressive transformers, combined with agent systems, will be able to independently complete highly nontrivial pure math work (perhaps still requiring mathematicians to verify the correctness of the final proof).</p> <h2 id="background-enumerative-geometry">Background: Enumerative Geometry</h2> <p>First, let me introduce the field of this paper: <strong>enumerative geometry</strong>. This is the purest of pure math, one of the mainstreams in pure mathematics. To understand intuitively, the questions this field studies include: how many intersection points does one geometric object (such as a curve) have with another geometric object (such as a surface). This field has profound connections with branches of theoretical physics (such as string theory).</p> <p>The last author of this work is <strong>Ravi Vakil</strong>. He is a full professor in Stanford’s mathematics department and president of the American Mathematical Society—a giant in enumerative geometry. His introductory algebraic geometry text <em>The Rising Sea</em> is extremely popular in this field; I spent nearly a year seriously studying it. It’s really well written (much better than Hartshorne). As for Vakil’s specific mathematical contributions, I’m not qualified to comment. But he’s definitely in the top 0.1% of the current mathematician community.</p> <h2 id="how-ai-participated-in-the-mathematical-proof">How AI Participated in the Mathematical Proof</h2> <p>The main theorem proved in this paper is <strong>Theorem 1.1</strong>:</p> <blockquote> <p><strong>Theorem 1.1.</strong> Suppose \(\beta = (d_1, \ldots, d_n)\) is strictly monotonic. Then we have the following equality in \(K_0(\text{Var}_{\mathbb{k}})\), the Grothendieck group of varieties over \(\mathbb{k}\):</p> \[[\Omega^2_{d_n,\ldots,d_1}(\text{Fl}_{n+1})] = \left[ \text{GL}_n \times \mathbb{A}^{D-n^2} \right]\] <p>where \(D = \sum_{k=1}^{n} 2d_k\).</p> </blockquote> <p>The mathematicians state in the paper that they have not seen anyone research this theorem before. The way AI proved this theorem differs from proofs of similar existing theorems. The authors of the paper writes:</p> <blockquote> <p>So, absent some future discovery to the contrary, the model’s contribution appears to involve a genuine combination of synthesis, retrieval, generalization and innovation of these existing techniques.</p> </blockquote> <h3 id="explaining-the-theorem-in-plain-language">Explaining the Theorem in Plain Language</h3> <p>The main object this paper studies is \(\Omega^2_{d_n,\ldots,d_1}(\text{Fl}_{n+1})\). Simply put, this is a complex geometric object.</p> <p>Given \((d_1,\ldots,d_n)\), \(\Omega^2_{d_n,\ldots,d_1}(\text{Fl}_{n+1})\) is a complex geometric object. When \(n\) and \(d_1,\ldots,d_n\) are all small (for example \(n=2, d_1=2, d_2=3\)), this geometric object is relatively simple. As \(n\) gets larger and \(d_1,\ldots,d_n\) get larger, this geometric object becomes increasingly complex.</p> <p>In contrast, \(GL_n \times \mathbb{A}^{D-n^2}\) is a very simple geometric object.</p> <p>\([\Omega^2_{d_n,\ldots,d_1}(\text{Fl}_{n+1})]\) refers to a special <strong>algebraic invariant</strong> of the object \(\Omega^2_{d_n,\ldots,d_1}(\text{Fl}_{n+1})\). An algebraic invariant can be understood as a geometric feature of this object that can be expressed numerically—for example, the number of holes in a geometric object is an algebraic invariant. The dimension of a geometric object is also an algebraic invariant.</p> <p>So the theorem states that a certain algebraic invariant of a complex geometric object, \(\Omega^2_{d_n,\ldots,d_1}(\text{Fl}_{n+1})\), equals the same algebraic invariant of a very simple geometric object, \(GL_n \times \mathbb{A}^{D-n^2}\). This equality holds for any strictly increasing sequence \((d_1,\ldots,d_n)\).</p> <p>This is a very typical enumerative geometry theorem: for a complex geometric object, we can compute a certain algebraic invariant, thereby gaining deeper understanding of its geometric features and thus better understanding the complex geometric object itself.</p> <p>One more note: don’t think “algebraic invariants” and “complex geometric objects” have nothing to do with reality. Under certain conditions, these geometric objects and invariants have very concrete physical meaning. For example, Yang-Mills theory and string theory can and must be described using the language of geometric objects and geometric invariants.</p> <p>Enumerative geometry and algebraic geometry are so profound that I once seriously considered whether to spend my entire career researching them.</p> <h3 id="the-proof-process">The Proof Process</h3> <p>The AI tools that participated in the proof include: <strong>Gemini 2.5 Deep Thinking</strong> and an unpublished Gemini-based tool called <strong>FullProof</strong> (likely a Gemini-based agent). The paper does not specify exactly how these two tools respectively contributed.</p> <p><strong>Proof process:</strong></p> <ol> <li> <p>The mathematicians proposed a conjecture: they first used algorithms to compute many special cases (for relatively small \(n\) and \(d_1,\ldots,d_n\)). They conjectured that <strong>Corollary A.1</strong>, a slightly weaker statement than Theorem 1.1, holds.</p> </li> <li> <p>The mathematicians first asked AI to prove whether Corollary A.1 holds when \(n\) and \(d_1,\ldots,d_n\) are relatively small (when the geometric object is relatively simple).</p> </li> <li> <p><strong>The AI system successfully proved these simple cases.</strong></p> </li> <li> <p>In AI’s proof, there was a <strong>key step containing important insight</strong>. Vakil stated that this key step is <em>profound</em>, <em>nontrivial</em>, and <em>he would be proud if he had proposed it</em>. The mathematicians carefully analyzed this key step. Based on it, they proposed a new proposition (which the mathematicians did not prove themselves), along with an approach for using this proposition to prove the main theorem.</p> </li> <li> <p>They put these into the prompt and asked AI to prove Corollary A.1. After checking AI’s proof, the mathematicians concluded that AI successfully proved Corollary A.1.</p> </li> <li> <p>The mathematicians then asked AI to prove Theorem 1.1 based on everything above. After checking AI’s proof, the mathematicians concluded that AI successfully proved Theorem 1.1.</p> </li> </ol> <p>Throughout this process, I’m unclear how much the mathematicians’ contribution—the new proposition they proposed based on AI’s key insight, and the approach to proving Corollary A.1—accounts for in the overall proof. I hope enumerative geometry experts can clarify this.</p> <h2 id="assessing-ais-capabilities-in-mathematical-proof">Assessing AI’s Capabilities in Mathematical Proof</h2> <p>Based on the above, we can conclude that:</p> <p>Commercial large language models available to the public in 2025, combined with agent systems, can in many mathematical fields:</p> <ul> <li><strong>Propose new, substantial, non-trivial mathematical insights</strong></li> <li><strong>Sufficiently understand cutting-edge pure math research</strong>. This is evidenced by the fact that in this paper, the model can write the correct proofs of non-trivial mathematical propositions when prompted with a general approach and related information.</li> </ul> <h2 id="predicting-ais-development-in-mathematical-proof">Predicting AI’s Development in Mathematical Proof</h2> <p>In February 2025, we believed SOTA models were essentially producing nonsense on pure math problems. For basic graduate-level math exercises, models would still make the most basic errors (like easy counting mistakes).</p> <p>By December 2025, SOTA models can already propose non-trivial mathematical insights in mainstream cutting-edge mathematical fields, and given a general approach, can write complete correct proofs of non-trivial mathematical propositions—meaning they have nontrivial understanding for frontier mathematics.</p> <p>I believe that <strong>SOTA models’ mathematical capabilities are far from reaching a bottleneck</strong>.</p> <p>On one hand, mainstream data curation and the SFT+RL post-training paradigm still have much room for exploration. Especially in data curation: since current SOTA models already have the ability to understand cutting-edge mathematics, we can use SOTA models to construct higher quality data based on existing mathematical data. Models can also further improve themselves by letting themselves check their own proof/proposed solution. Remember this paper proves that model can make nontrivial attempt in proving, and it isn’t a far fetched hypothesis that checking once own proof is an easier task than producing a good proof.</p> <p>On the other hand, formal languages like <strong>Lean</strong> have most likely not yet been truly applied to SOTA model training.</p> <p>Therefore, I believe that <strong>models’ mathematical capabilities will continue to progress in 2026 at a speed faster than the progress from February 2025 to December 2025</strong>.</p> <h2 id="suggestions-for-mathematicians">Suggestions for Mathematicians</h2> <p>When writing mathematical works in collaboration with AI, what information should be included to help the community better understand AI’s capabilities in mathematics?</p> <p>This paper does not explain:</p> <ul> <li>Can the model realize after many attempts that its initially proposed key step is crucial?</li> <li>If prompted with the initially proposed key step, can the model through extensive experimentation find the new key proposition and the approach that mathematicians proposed after analysis?</li> <li>Can the model judge whether its own proof is correct without help from mathematicians?</li> <li>How many attempts did the model make to achieve each critical steps in the paper?</li> </ul> <p><strong>I hope future papers on how AI helps prove math theorems will include this information. This will help us better assess AI models’ capabilities.</strong></p>]]></content><author><name></name></author><category term="paper-analysis"/><category term="AI"/><category term="math"/><category term="reasoning"/><summary type="html"><![CDATA[Analyzing how autoregressive AI models contributed non-trivial mathematical insights to an open problem in algebraic geometry]]></summary></entry></feed>